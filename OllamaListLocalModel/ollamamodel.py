#!/usr/bin/env python3
"""
M√©thodes pour lister les mod√®les Ollama locaux
"""

import requests
import json
from typing import List, Dict


def get_ollama_models(base_url: str = "http://localhost:11434") -> List[str]:
    """
    R√©cup√®re la liste des noms de mod√®les Ollama disponibles localement
    
    Args:
        base_url: URL du serveur Ollama (d√©faut: http://localhost:11434)
    
    Returns:
        Liste des noms de mod√®les disponibles
    
    Example:
        >>> models = get_ollama_models()
        >>> print(models)
        ['llama3', 'mistral', 'codellama']
    """
    try:
        response = requests.get(f"{base_url}/api/tags")
        response.raise_for_status()
        
        data = response.json()
        models = [model['name'] for model in data.get('models', [])]
        
        return models
    
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Erreur: Impossible de se connecter √† Ollama sur {base_url}")
        print("   Assurez-vous qu'Ollama est d√©marr√© (ollama serve)")
        return []
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration des mod√®les: {e}")
        return []


def get_ollama_models_detailed(base_url: str = "http://localhost:11434") -> List[Dict]:
    """
    R√©cup√®re la liste d√©taill√©e des mod√®les Ollama avec leurs informations
    
    Args:
        base_url: URL du serveur Ollama (d√©faut: http://localhost:11434)
    
    Returns:
        Liste de dictionnaires contenant les d√©tails de chaque mod√®le
    
    Example:
        >>> models = get_ollama_models_detailed()
        >>> for model in models:
        ...     print(f"{model['name']} - {model['size_gb']:.2f} GB")
    """
    try:
        response = requests.get(f"{base_url}/api/tags")
        response.raise_for_status()
        
        data = response.json()
        models_info = []
        
        for model in data.get('models', []):
            model_info = {
                'name': model['name'],
                'modified_at': model.get('modified_at', 'N/A'),
                'size': model.get('size', 0),
                'size_gb': model.get('size', 0) / (1024**3),  # Conversion en GB
                'digest': model.get('digest', 'N/A')[:12],  # Premiers caract√®res
            }
            models_info.append(model_info)
        
        return models_info
    
    except requests.exceptions.ConnectionError:
        print(f"‚ùå Erreur: Impossible de se connecter √† Ollama sur {base_url}")
        print("   Assurez-vous qu'Ollama est d√©marr√© (ollama serve)")
        return []
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration des mod√®les: {e}")
        return []


def print_available_models(base_url: str = "http://localhost:11434"):
    """
    Affiche la liste des mod√®les Ollama disponibles de mani√®re format√©e
    
    Args:
        base_url: URL du serveur Ollama (d√©faut: http://localhost:11434)
    
    Example:
        >>> print_available_models()
        Mod√®les Ollama disponibles:
        ============================
        1. llama3 (4.2 GB)
        2. mistral (7.1 GB)
        3. codellama (3.8 GB)
    """
    models = get_ollama_models_detailed(base_url)
    
    if not models:
        print("Aucun mod√®le trouv√© ou Ollama n'est pas accessible.")
        return
    
    print("\nMod√®les Ollama disponibles:")
    print("=" * 60)
    
    for i, model in enumerate(models, 1):
        size_str = f"{model['size_gb']:.2f} GB" if model['size_gb'] > 0 else "Taille inconnue"
        print(f"{i}. {model['name']:30} ({size_str})")
    
    print("=" * 60)
    print(f"Total: {len(models)} mod√®le(s)\n")


def check_model_exists(model_name: str, base_url: str = "http://localhost:11434") -> bool:
    """
    V√©rifie si un mod√®le sp√©cifique existe localement
    
    Args:
        model_name: Nom du mod√®le √† v√©rifier
        base_url: URL du serveur Ollama
    
    Returns:
        True si le mod√®le existe, False sinon
    
    Example:
        >>> if check_model_exists("llama3"):
        ...     print("Mod√®le disponible!")
    """
    models = get_ollama_models(base_url)
    return model_name in models


# ============================================================================
# Exemple d'utilisation
# ============================================================================

if __name__ == "__main__":
    print("ü¶ô V√©rification des mod√®les Ollama locaux\n")
    
    # M√©thode 1: Liste simple des noms
    print("üìã Liste simple:")
    models = get_ollama_models()
    if models:
        for model in models:
            print(f"  - {model}")
    print()
    
    # M√©thode 2: Liste d√©taill√©e format√©e
    print_available_models()
    
    # M√©thode 3: V√©rifier un mod√®le sp√©cifique
    model_to_check = "llama3"
    if check_model_exists(model_to_check):
        print(f"‚úì Le mod√®le '{model_to_check}' est disponible")
    else:
        print(f"‚úó Le mod√®le '{model_to_check}' n'est pas install√©")
        print(f"  Installez-le avec: ollama pull {model_to_check}")
    
    print()
    
    # M√©thode 4: Informations d√©taill√©es
    print("üìä Informations d√©taill√©es:")
    detailed_models = get_ollama_models_detailed()
    for model in detailed_models:
        print(f"\n  Mod√®le: {model['name']}")
        print(f"  Taille: {model['size_gb']:.2f} GB")
        print(f"  Modifi√©: {model['modified_at']}")
        print(f"  Digest: {model['digest']}")
